# DK-DDIL: Adaptive Knowledge Retention for Dynamic Domain-Incremental Learning in Medical Imaging

[![CVPR 2026](https://img.shields.io/badge/CVPR-2026-blue.svg)](https://cvpr.thecvf.com/)
[![arXiv](https://img.shields.io/badge/arXiv-xxxx.xxxxx-b31b1b)]()
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

ğŸ“„ **Paper**: [DK-DDIL: Adaptive Knowledge Retention for Dynamic Domain-Incremental Learning in Medical Imaging]()

## Overview

DK-DDIL is a rehearsal-free dynamic domain-incremental learning framework designed for evolving medical imaging scenarios. Clinical imaging data continuously evolves due to new scanners, institutions, and disease subtypes. Traditional DIL methods assume:

- Fixed label spaces
- Limited domain shifts

DK-DDIL enables dynamic domain adaptation without rehearsal 
via two synergistic modules:

1. ğŸ”„**Dynamic Adaptation Module (DAM)**
   - Dynamic rank selection
   - Adaptive regularization

2. ğŸ§ **Knowledge Inheritance & Refinement (KIR)**
   - Selective adapter fusion
   - Prototype-level contrastive refinement

Our method consistently outperforms SOTA DIL methods across 2D, 3D medical imaging, and natural image benchmarks.

<p align="center">
  <img src="figures/dil_mainfig.jpg" width="80%">
</p>

## Environment
You can create a conda environment and run the following command to install the dependencies.
```
conda create -n DK-DDIL python=3.10 -y
conda activate DK-DDIL
pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

## Datasets Preparation

There are **3** datasets involved in the paper, *Skin Pathology Diagnosis*, *Cyst-X*and *Office-Home* respectively. Follow the two-step guideline to prepare them for the reproduction.

### 1. Download datasets ###
Download the datasets mannually according the recommended.
    - **Skin Pathology Diagnosis**: Please download the data for each center using the links provided below.  
      - *PH2*: [link](https://huggingface.co/datasets/Shah1st/PH2).  
      - *BCN*:[link](https://figshare.com/articles/journal_contribution/BCN20000_Dermoscopic_Lesions_in_the_Wild/24140028/1).  
      - *HAMã€MSKã€OTHER*:[link](https://challenge.isic-archive.com/data/#2019).  
      - *derm_D7P*:[link](https://derm.cs.sfu.ca/Download.html)  
      - *dermoscopic*:[link](https://api.isic-archive.com/doi/milk10k/)  
    - **Cyst-X**: You can access the dataset at this [link](https://osf.io/74vfs/overview). Please download `IPMN_Classification/t1_clean_ROI.zip` and `IPMN_labels_t1_total.csv` file.  
    - **Office-Home**: You can access the dataset at this [link](https://hemanthdv.github.io/officehome-dataset/).  

### 2. Data Preprocessing ###
We first organize the datasets into domains and corresponding classes for each domain as follows:  
    - **Skin Pathology Diagnosis**:   
      - ***PH2***:Based on the image data and descriptions stored in the Parquet files, images are categorized into two lesion types: NV and MEL.  
      - ***BCN***:Images are grouped according to the "diagnosis" field in the provided CSV file.  
      - ***HAMã€MSK***:From the ISIC 2019 Challenge training set, samples from the HAM, MSK, and BCN centers are selected according to the "lesion_id" field in meta.csv. 
      Samples whose "lesion_id" is neither NaN nor associated with HAM/MSK/BCN are merged into the MSK domain. 
      The HAM and MSK samples are then categorized based on the labels provided in GroundTruth.csv.
      - ***OTHER***:The ISIC 2019 test set is treated as a separate domain named OTHER.  
      - ***derm_D7P***:The "diagnosis" field in meta.csv is first mapped to our predefined skin lesion categories. Samples corresponding to the "derm" column are then organized according to these mapped labels.  
      - ***dermoscopic***:Samples are selected based on the "image_type" field in meta.csv to construct the dermoscopic domain. These samples are categorized according to their labels in gt.csv.  
    - **Cyst-X**: In IPMN_labels_t1_total.csv, the "Patient ID" field corresponds to the filename of each sample. Based on the filename prefix, samples are grouped into different centers (e.g., AHN, MCF). The "risk assessment" field is used as the class label for categorization.  
    - **Office-Home**: The original dataset contains four different artistic styles, which naturally correspond to four domains. The existing folder structure already matches our required dataset organization.  
    **Dataset Splitting**: For each domain, we adopt stratified sampling to partition the dataset into training and testing subsets with a ratio of 4:1, preserving the class distribution within each domain.  

<!-- æ•´ç†æ•°æ®é›†çš„domainsä»¥åŠæ¯ä¸ªdomainçš„classesï¼š
    - **Skin Pathology Diagnosis**: 
      - *PH2*:æ ¹æ®Parquetæ–‡ä»¶ä¸­å­˜å‚¨çš„å›¾åƒå’Œdescriptionï¼Œå°†å›¾åƒåˆ†ä¸ºNVå’ŒMELä¸¤ç§ç—…å˜ç±»åˆ«ï¼›
      - *BCN*:æ ¹æ®csvæ–‡ä»¶çš„"diagnosis"å­—æ®µå°†å›¾åƒåˆ†ç±»å­˜æ”¾ï¼›
      - *HAMã€MSK*:åœ¨ISIC 2019 Challengeçš„è®­ç»ƒé›†ä¸­ï¼Œæ ¹æ®meta.csvä¸­çš„"lession_id"å­—æ®µç­›é€‰å‡ºHAMã€MSKã€BCNä¸‰ä¸ªä¸­å¿ƒçš„æ ·æœ¬ï¼Œå¯¹äº"lession_id"å­—æ®µæ—¢ä¸æ˜¯nanï¼Œä¹Ÿä¸å±äºMSK/BCN/HAMçš„æ ·æœ¬ï¼Œåˆå¹¶åˆ°MSKä¸­å¿ƒï¼Œè€ŒåæŒ‰ç…§GroundTruth.csvä¸­çš„å­—æ®µå°†HAMã€MSKçš„æ ·æœ¬åˆ†ç±»å­˜æ”¾ã€‚
      - *OTHER*:å°†ISIC 2019çš„æµ‹è¯•é›†ä½œä¸ºOTHERä¸­å¿ƒçš„æ ·æœ¬ã€‚
      - *derm_D7P*:é¦–å…ˆå°†meta.csvä¸­çš„"diagnosis"å­—æ®µè½¬åŒ–ä¸ºæˆ‘ä»¬è®¾å®šçš„çš®è‚¤ç—…å˜æ ‡ç­¾ï¼Œè€Œåå°†"derm"åˆ—å¯¹åº”çš„æ ·æœ¬åˆ†ç±»å­˜æ”¾ã€‚
      - *dermoscopic*:æ ¹æ®meta.csvä¸­çš„"image_type"å­—æ®µç­›é€‰å‡ºdermoscopicä¸­å¿ƒçš„æ ·æœ¬ï¼Œè€Œåå°†è¿™äº›æ ·æœ¬æŒ‰ç…§gt.csvä¸­å¯¹åº”çš„æ ‡ç­¾åˆ†ç±»å­˜æ”¾ã€‚
    - **Cyst-X**: IPMN_labels_t1_total.csvä¸­çš„"Patient ID"å­—æ®µå¯¹åº”æ¯ä¸ªæ ·æœ¬çš„æ–‡ä»¶åï¼Œæ ¹æ®å…¶æ–‡ä»¶åå‰ç¼€ï¼Œå°†æ ·æœ¬åˆ†ä¸ºAHNã€MCFç­‰ä¸­å¿ƒï¼Œè€Œåå°†"risk assessment"å­—æ®µä½œä¸ºæ ‡ç­¾ï¼Œå°†æ ·æœ¬åˆ†ç±»å­˜æ”¾ã€‚
    - **Office-Home**: åŸæ•°æ®é›†æ–‡ä»¶å¤¹ä¸‹æœ‰å››ç§ä¸åŒé£æ ¼ï¼Œå³å››ä¸ªä¸åŒçš„domainï¼Œå·²ç»ç¬¦åˆæˆ‘ä»¬çš„æ•°æ®é›†ç»“æ„ã€‚
    **Dataset Splitting**: å¯¹æ¯ä¸ªdomainï¼ŒæŒ‰ç…§åˆ†å±‚æŠ½æ ·çš„æ–¹å¼ï¼Œå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†:æµ‹è¯•é›†=4:1ã€‚ -->

### 3. Check structure ###
Check if the dataset has been downloaded properly and place the dataset files in the `./data/` directory. The dataset directory is expected to have the following structure:  
```
    Skin
    â”œâ”€â”€ BCN
      â”œâ”€â”€ class0
        â”œâ”€â”€ images
      â”œâ”€â”€ class1
      â”œâ”€â”€ ...(other classes)
    â”œâ”€â”€ BCN_test.txt
    â”œâ”€â”€ BCN_train.txt
    â”œâ”€â”€ ...(other domains)

    Cyst-X
    â”œâ”€â”€ AHN
      â”œâ”€â”€ class0
        â”œâ”€â”€ images
      â”œâ”€â”€ class1
      â”œâ”€â”€ ...(other classes)
    â”œâ”€â”€ AHN_test.txt
    â”œâ”€â”€ AHN_train.txt
    â”œâ”€â”€ ...(other domains)

    OfficeHome
    â”œâ”€â”€ Art
      â”œâ”€â”€ class0
        â”œâ”€â”€ images
      â”œâ”€â”€ class1
      â”œâ”€â”€ ...(other classes)    
    â”œâ”€â”€ Art_test.txt
    â”œâ”€â”€ Art_train.txt
    â”œâ”€â”€ ...(other domains)
```  


## Getting Started

### Training

To train the model, navigate to the main directory and run:

```
python main.py --config <json_config_path>
```

### Supported Datasets

The following datasets are supported with pre-configured JSON files:

#### Skin Pathology Diagnosis
```
python main.py --config exps/skin.json
```
#### Cyst-X
```
python main.py --config exps/cystx.json
```
#### Office-Home
```
python main.py --config exps/officehome.json
```


## Citation

If you find this work useful in your research, please cite:

```bibtex
@article{yxma_2026_CVPR,
    author    = {},
    title     = {DK-DDIL: Adaptive Knowledge Retention for Dynamic Domain-Incremental Learning in Medical Imaging},
    journal = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},
    month     = {June},
    year      = {2026},
    pages     = {}
}
```

## Acknowledgments

This implementation builds upon the **LAMDA-PILOT** and **CL-LoRA** framework. 

**LAMDA-PILOT Repository**: https://github.com/sun-hailong/LAMDA-PILOT
**CL-LoRA Repository**: https://github.com/JiangpengHe/CL-LoRA

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact

For questions or issues, please open an issue on GitHub or contact the authors.
