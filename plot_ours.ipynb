{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxma/anaconda3/envs/cllora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "import torch\n",
    "import json\n",
    "from utils.inc_net import OurNet\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(setting_path):\n",
    "    with open(setting_path) as data_file:\n",
    "        param = json.load(data_file)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "I'm using ViT with adapters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxma/anaconda3/envs/cllora/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/yxma/anaconda3/envs/cllora/lib/python3.10/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name vit_base_patch16_224_in21k to current vit_base_patch16_224.augreg_in21k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['block_weight', 'cur_adapter.0.0.lora_A.weight', 'cur_adapter.0.0.lora_B.weight', 'cur_adapter.0.1.lora_A.weight', 'cur_adapter.0.1.lora_B.weight', 'cur_adapter.0.2.lora_A.weight', 'cur_adapter.0.2.lora_B.weight', 'cur_adapter.0.3.lora_A.weight', 'cur_adapter.0.3.lora_B.weight', 'cur_adapter.1.0.lora_A.weight', 'cur_adapter.1.0.lora_B.weight', 'cur_adapter.1.1.lora_A.weight', 'cur_adapter.1.1.lora_B.weight', 'cur_adapter.1.2.lora_A.weight', 'cur_adapter.1.2.lora_B.weight', 'cur_adapter.1.3.lora_A.weight', 'cur_adapter.1.3.lora_B.weight', 'cur_adapter.2.0.lora_A.weight', 'cur_adapter.2.0.lora_B.weight', 'cur_adapter.2.1.lora_A.weight', 'cur_adapter.2.1.lora_B.weight', 'cur_adapter.2.2.lora_A.weight', 'cur_adapter.2.2.lora_B.weight', 'cur_adapter.2.3.lora_A.weight', 'cur_adapter.2.3.lora_B.weight', 'cur_adapter.3.0.lora_A.weight', 'cur_adapter.3.0.lora_B.weight', 'cur_adapter.3.1.lora_A.weight', 'cur_adapter.3.1.lora_B.weight', 'cur_adapter.3.2.lora_A.weight', 'cur_adapter.3.2.lora_B.weight', 'cur_adapter.3.3.lora_A.weight', 'cur_adapter.3.3.lora_B.weight', 'cur_adapter.4.0.lora_A.weight', 'cur_adapter.4.0.lora_B.weight', 'cur_adapter.4.1.lora_A.weight', 'cur_adapter.4.1.lora_B.weight', 'cur_adapter.4.2.lora_A.weight', 'cur_adapter.4.2.lora_B.weight', 'cur_adapter.4.3.lora_A.weight', 'cur_adapter.4.3.lora_B.weight', 'cur_adapter.5.0.lora_A.weight', 'cur_adapter.5.0.lora_B.weight', 'cur_adapter.5.1.lora_A.weight', 'cur_adapter.5.1.lora_B.weight', 'cur_adapter.5.2.lora_A.weight', 'cur_adapter.5.2.lora_B.weight', 'cur_adapter.5.3.lora_A.weight', 'cur_adapter.5.3.lora_B.weight', 'cur_adapter.6.0.lora_A.weight', 'cur_adapter.6.0.lora_B.weight', 'cur_adapter.6.1.lora_A.weight', 'cur_adapter.6.1.lora_B.weight', 'cur_adapter.6.2.lora_A.weight', 'cur_adapter.6.2.lora_B.weight', 'cur_adapter.6.3.lora_A.weight', 'cur_adapter.6.3.lora_B.weight', 'cur_adapter.7.0.lora_A.weight', 'cur_adapter.7.0.lora_B.weight', 'cur_adapter.7.1.lora_A.weight', 'cur_adapter.7.1.lora_B.weight', 'cur_adapter.7.2.lora_A.weight', 'cur_adapter.7.2.lora_B.weight', 'cur_adapter.7.3.lora_A.weight', 'cur_adapter.7.3.lora_B.weight', 'cur_adapter.8.0.lora_A.weight', 'cur_adapter.8.0.lora_B.weight', 'cur_adapter.8.1.lora_A.weight', 'cur_adapter.8.1.lora_B.weight', 'cur_adapter.8.2.lora_A.weight', 'cur_adapter.8.2.lora_B.weight', 'cur_adapter.8.3.lora_A.weight', 'cur_adapter.8.3.lora_B.weight', 'cur_adapter.9.0.lora_A.weight', 'cur_adapter.9.0.lora_B.weight', 'cur_adapter.9.1.lora_A.weight', 'cur_adapter.9.1.lora_B.weight', 'cur_adapter.9.2.lora_A.weight', 'cur_adapter.9.2.lora_B.weight', 'cur_adapter.9.3.lora_A.weight', 'cur_adapter.9.3.lora_B.weight', 'cur_adapter.10.0.lora_A.weight', 'cur_adapter.10.0.lora_B.weight', 'cur_adapter.10.1.lora_A.weight', 'cur_adapter.10.1.lora_B.weight', 'cur_adapter.10.2.lora_A.weight', 'cur_adapter.10.2.lora_B.weight', 'cur_adapter.10.3.lora_A.weight', 'cur_adapter.10.3.lora_B.weight', 'cur_adapter.11.0.lora_A.weight', 'cur_adapter.11.0.lora_B.weight', 'cur_adapter.11.1.lora_A.weight', 'cur_adapter.11.1.lora_B.weight', 'cur_adapter.11.2.lora_A.weight', 'cur_adapter.11.2.lora_B.weight', 'cur_adapter.11.3.lora_A.weight', 'cur_adapter.11.3.lora_B.weight'], unexpected_keys=[])\n",
      "After BaseNet initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OurNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (old_adapter_list): ModuleList()\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention_lora(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (pre_logits): Identity()\n",
       "    (head): Identity()\n",
       "    (cur_adapter): ModuleList(\n",
       "      (0-11): 12 x ModuleList(\n",
       "        (0-3): 4 x Adapter_lora(\n",
       "          (lora_A): Linear(in_features=8, out_features=768, bias=False)\n",
       "          (lora_B): Linear(in_features=768, out_features=8, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_list): ModuleList()\n",
       "  (fc_list_task): ModuleList()\n",
       "  (adapter_list): ModuleList()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = load_json(\"exps/skin_ours.json\")\n",
    "param[\"device\"] = [\"cpu\"]\n",
    "network = OurNet(param, True)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ckpt_dir = \"logs/ours/SKIN/2/3/no_kd_orth_all_spec_lora\"\n",
    "task_best_model_ckpts = []\n",
    "for task in range(9):\n",
    "    task_ckpt_root = os.path.join(task_ckpt_dir,f\"best_model_task_{task}.pth\")\n",
    "    task_best_model_ckpts.append(torch.load(task_ckpt_root)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbone.block_weight', 'backbone.cls_token', 'backbone.pos_embed', 'backbone.patch_embed.proj.weight', 'backbone.patch_embed.proj.bias', 'backbone.blocks.0.norm1.weight', 'backbone.blocks.0.norm1.bias', 'backbone.blocks.0.attn.q_proj.weight', 'backbone.blocks.0.attn.q_proj.bias', 'backbone.blocks.0.attn.v_proj.weight', 'backbone.blocks.0.attn.v_proj.bias', 'backbone.blocks.0.attn.k_proj.weight', 'backbone.blocks.0.attn.k_proj.bias', 'backbone.blocks.0.attn.proj.weight', 'backbone.blocks.0.attn.proj.bias', 'backbone.blocks.0.norm2.weight', 'backbone.blocks.0.norm2.bias', 'backbone.blocks.0.fc1.weight', 'backbone.blocks.0.fc1.bias', 'backbone.blocks.0.fc2.weight', 'backbone.blocks.0.fc2.bias', 'backbone.blocks.1.norm1.weight', 'backbone.blocks.1.norm1.bias', 'backbone.blocks.1.attn.q_proj.weight', 'backbone.blocks.1.attn.q_proj.bias', 'backbone.blocks.1.attn.v_proj.weight', 'backbone.blocks.1.attn.v_proj.bias', 'backbone.blocks.1.attn.k_proj.weight', 'backbone.blocks.1.attn.k_proj.bias', 'backbone.blocks.1.attn.proj.weight', 'backbone.blocks.1.attn.proj.bias', 'backbone.blocks.1.norm2.weight', 'backbone.blocks.1.norm2.bias', 'backbone.blocks.1.fc1.weight', 'backbone.blocks.1.fc1.bias', 'backbone.blocks.1.fc2.weight', 'backbone.blocks.1.fc2.bias', 'backbone.blocks.2.norm1.weight', 'backbone.blocks.2.norm1.bias', 'backbone.blocks.2.attn.q_proj.weight', 'backbone.blocks.2.attn.q_proj.bias', 'backbone.blocks.2.attn.v_proj.weight', 'backbone.blocks.2.attn.v_proj.bias', 'backbone.blocks.2.attn.k_proj.weight', 'backbone.blocks.2.attn.k_proj.bias', 'backbone.blocks.2.attn.proj.weight', 'backbone.blocks.2.attn.proj.bias', 'backbone.blocks.2.norm2.weight', 'backbone.blocks.2.norm2.bias', 'backbone.blocks.2.fc1.weight', 'backbone.blocks.2.fc1.bias', 'backbone.blocks.2.fc2.weight', 'backbone.blocks.2.fc2.bias', 'backbone.blocks.3.norm1.weight', 'backbone.blocks.3.norm1.bias', 'backbone.blocks.3.attn.q_proj.weight', 'backbone.blocks.3.attn.q_proj.bias', 'backbone.blocks.3.attn.v_proj.weight', 'backbone.blocks.3.attn.v_proj.bias', 'backbone.blocks.3.attn.k_proj.weight', 'backbone.blocks.3.attn.k_proj.bias', 'backbone.blocks.3.attn.proj.weight', 'backbone.blocks.3.attn.proj.bias', 'backbone.blocks.3.norm2.weight', 'backbone.blocks.3.norm2.bias', 'backbone.blocks.3.fc1.weight', 'backbone.blocks.3.fc1.bias', 'backbone.blocks.3.fc2.weight', 'backbone.blocks.3.fc2.bias', 'backbone.blocks.4.norm1.weight', 'backbone.blocks.4.norm1.bias', 'backbone.blocks.4.attn.q_proj.weight', 'backbone.blocks.4.attn.q_proj.bias', 'backbone.blocks.4.attn.v_proj.weight', 'backbone.blocks.4.attn.v_proj.bias', 'backbone.blocks.4.attn.k_proj.weight', 'backbone.blocks.4.attn.k_proj.bias', 'backbone.blocks.4.attn.proj.weight', 'backbone.blocks.4.attn.proj.bias', 'backbone.blocks.4.norm2.weight', 'backbone.blocks.4.norm2.bias', 'backbone.blocks.4.fc1.weight', 'backbone.blocks.4.fc1.bias', 'backbone.blocks.4.fc2.weight', 'backbone.blocks.4.fc2.bias', 'backbone.blocks.5.norm1.weight', 'backbone.blocks.5.norm1.bias', 'backbone.blocks.5.attn.q_proj.weight', 'backbone.blocks.5.attn.q_proj.bias', 'backbone.blocks.5.attn.v_proj.weight', 'backbone.blocks.5.attn.v_proj.bias', 'backbone.blocks.5.attn.k_proj.weight', 'backbone.blocks.5.attn.k_proj.bias', 'backbone.blocks.5.attn.proj.weight', 'backbone.blocks.5.attn.proj.bias', 'backbone.blocks.5.norm2.weight', 'backbone.blocks.5.norm2.bias', 'backbone.blocks.5.fc1.weight', 'backbone.blocks.5.fc1.bias', 'backbone.blocks.5.fc2.weight', 'backbone.blocks.5.fc2.bias', 'backbone.blocks.6.norm1.weight', 'backbone.blocks.6.norm1.bias', 'backbone.blocks.6.attn.q_proj.weight', 'backbone.blocks.6.attn.q_proj.bias', 'backbone.blocks.6.attn.v_proj.weight', 'backbone.blocks.6.attn.v_proj.bias', 'backbone.blocks.6.attn.k_proj.weight', 'backbone.blocks.6.attn.k_proj.bias', 'backbone.blocks.6.attn.proj.weight', 'backbone.blocks.6.attn.proj.bias', 'backbone.blocks.6.norm2.weight', 'backbone.blocks.6.norm2.bias', 'backbone.blocks.6.fc1.weight', 'backbone.blocks.6.fc1.bias', 'backbone.blocks.6.fc2.weight', 'backbone.blocks.6.fc2.bias', 'backbone.blocks.7.norm1.weight', 'backbone.blocks.7.norm1.bias', 'backbone.blocks.7.attn.q_proj.weight', 'backbone.blocks.7.attn.q_proj.bias', 'backbone.blocks.7.attn.v_proj.weight', 'backbone.blocks.7.attn.v_proj.bias', 'backbone.blocks.7.attn.k_proj.weight', 'backbone.blocks.7.attn.k_proj.bias', 'backbone.blocks.7.attn.proj.weight', 'backbone.blocks.7.attn.proj.bias', 'backbone.blocks.7.norm2.weight', 'backbone.blocks.7.norm2.bias', 'backbone.blocks.7.fc1.weight', 'backbone.blocks.7.fc1.bias', 'backbone.blocks.7.fc2.weight', 'backbone.blocks.7.fc2.bias', 'backbone.blocks.8.norm1.weight', 'backbone.blocks.8.norm1.bias', 'backbone.blocks.8.attn.q_proj.weight', 'backbone.blocks.8.attn.q_proj.bias', 'backbone.blocks.8.attn.v_proj.weight', 'backbone.blocks.8.attn.v_proj.bias', 'backbone.blocks.8.attn.k_proj.weight', 'backbone.blocks.8.attn.k_proj.bias', 'backbone.blocks.8.attn.proj.weight', 'backbone.blocks.8.attn.proj.bias', 'backbone.blocks.8.norm2.weight', 'backbone.blocks.8.norm2.bias', 'backbone.blocks.8.fc1.weight', 'backbone.blocks.8.fc1.bias', 'backbone.blocks.8.fc2.weight', 'backbone.blocks.8.fc2.bias', 'backbone.blocks.9.norm1.weight', 'backbone.blocks.9.norm1.bias', 'backbone.blocks.9.attn.q_proj.weight', 'backbone.blocks.9.attn.q_proj.bias', 'backbone.blocks.9.attn.v_proj.weight', 'backbone.blocks.9.attn.v_proj.bias', 'backbone.blocks.9.attn.k_proj.weight', 'backbone.blocks.9.attn.k_proj.bias', 'backbone.blocks.9.attn.proj.weight', 'backbone.blocks.9.attn.proj.bias', 'backbone.blocks.9.norm2.weight', 'backbone.blocks.9.norm2.bias', 'backbone.blocks.9.fc1.weight', 'backbone.blocks.9.fc1.bias', 'backbone.blocks.9.fc2.weight', 'backbone.blocks.9.fc2.bias', 'backbone.blocks.10.norm1.weight', 'backbone.blocks.10.norm1.bias', 'backbone.blocks.10.attn.q_proj.weight', 'backbone.blocks.10.attn.q_proj.bias', 'backbone.blocks.10.attn.v_proj.weight', 'backbone.blocks.10.attn.v_proj.bias', 'backbone.blocks.10.attn.k_proj.weight', 'backbone.blocks.10.attn.k_proj.bias', 'backbone.blocks.10.attn.proj.weight', 'backbone.blocks.10.attn.proj.bias', 'backbone.blocks.10.norm2.weight', 'backbone.blocks.10.norm2.bias', 'backbone.blocks.10.fc1.weight', 'backbone.blocks.10.fc1.bias', 'backbone.blocks.10.fc2.weight', 'backbone.blocks.10.fc2.bias', 'backbone.blocks.11.norm1.weight', 'backbone.blocks.11.norm1.bias', 'backbone.blocks.11.attn.q_proj.weight', 'backbone.blocks.11.attn.q_proj.bias', 'backbone.blocks.11.attn.v_proj.weight', 'backbone.blocks.11.attn.v_proj.bias', 'backbone.blocks.11.attn.k_proj.weight', 'backbone.blocks.11.attn.k_proj.bias', 'backbone.blocks.11.attn.proj.weight', 'backbone.blocks.11.attn.proj.bias', 'backbone.blocks.11.norm2.weight', 'backbone.blocks.11.norm2.bias', 'backbone.blocks.11.fc1.weight', 'backbone.blocks.11.fc1.bias', 'backbone.blocks.11.fc2.weight', 'backbone.blocks.11.fc2.bias', 'backbone.norm.weight', 'backbone.norm.bias', 'backbone.cur_adapter.0.0.lora_A.weight', 'backbone.cur_adapter.0.0.lora_B.weight', 'backbone.cur_adapter.0.2.lora_A.weight', 'backbone.cur_adapter.0.2.lora_B.weight', 'backbone.cur_adapter.1.0.lora_A.weight', 'backbone.cur_adapter.1.0.lora_B.weight', 'backbone.cur_adapter.1.2.lora_A.weight', 'backbone.cur_adapter.1.2.lora_B.weight', 'backbone.cur_adapter.2.0.lora_A.weight', 'backbone.cur_adapter.2.0.lora_B.weight', 'backbone.cur_adapter.2.2.lora_A.weight', 'backbone.cur_adapter.2.2.lora_B.weight', 'backbone.cur_adapter.3.0.lora_A.weight', 'backbone.cur_adapter.3.0.lora_B.weight', 'backbone.cur_adapter.3.2.lora_A.weight', 'backbone.cur_adapter.3.2.lora_B.weight', 'backbone.cur_adapter.4.0.lora_A.weight', 'backbone.cur_adapter.4.0.lora_B.weight', 'backbone.cur_adapter.4.2.lora_A.weight', 'backbone.cur_adapter.4.2.lora_B.weight', 'backbone.cur_adapter.5.0.lora_A.weight', 'backbone.cur_adapter.5.0.lora_B.weight', 'backbone.cur_adapter.5.2.lora_A.weight', 'backbone.cur_adapter.5.2.lora_B.weight', 'backbone.cur_adapter.6.0.lora_A.weight', 'backbone.cur_adapter.6.0.lora_B.weight', 'backbone.cur_adapter.6.2.lora_A.weight', 'backbone.cur_adapter.6.2.lora_B.weight', 'backbone.cur_adapter.7.0.lora_A.weight', 'backbone.cur_adapter.7.0.lora_B.weight', 'backbone.cur_adapter.7.2.lora_A.weight', 'backbone.cur_adapter.7.2.lora_B.weight', 'backbone.cur_adapter.8.0.lora_A.weight', 'backbone.cur_adapter.8.0.lora_B.weight', 'backbone.cur_adapter.8.2.lora_A.weight', 'backbone.cur_adapter.8.2.lora_B.weight', 'backbone.cur_adapter.9.0.lora_A.weight', 'backbone.cur_adapter.9.0.lora_B.weight', 'backbone.cur_adapter.9.2.lora_A.weight', 'backbone.cur_adapter.9.2.lora_B.weight', 'backbone.cur_adapter.10.0.lora_A.weight', 'backbone.cur_adapter.10.0.lora_B.weight', 'backbone.cur_adapter.10.2.lora_A.weight', 'backbone.cur_adapter.10.2.lora_B.weight', 'backbone.cur_adapter.11.0.lora_A.weight', 'backbone.cur_adapter.11.0.lora_B.weight', 'backbone.cur_adapter.11.2.lora_A.weight', 'backbone.cur_adapter.11.2.lora_B.weight', 'proxy_fc.weight', 'proxy_fc.sigma', 'init_proto.weight', 'init_proto.sigma', 'fc.weight', 'fc.sigma'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_best_model_ckpts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lora_parameters_by_key(state_dict):\n",
    "    \"\"\"从状态字典中提取lora_a和lora_b参数\"\"\"\n",
    "    lora_a_params = {}\n",
    "    lora_b_params = {}\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        if 'lora_A' in key and 'weight' in key and 'cur_adapter' in key:\n",
    "            param_name = key.split('.')[-4] + '.' + key.split('.')[-3] + '.' + key.split('.')[-2]  # 提取更具体的参数名称\n",
    "            lora_a_params[param_name] = value\n",
    "        elif 'lora_B' in key and 'weight' in key and 'cur_adapter' in key:\n",
    "            param_name = key.split('.')[-4] + '.' + key.split('.')[-3] + '.' + key.split('.')[-2]\n",
    "            lora_b_params[param_name] = value\n",
    "    \n",
    "    return lora_a_params, lora_b_params\n",
    "\n",
    "def calculate_key_similarity_matrix(params_list, key_name):\n",
    "    \"\"\"计算特定key在所有任务间的相似度矩阵\"\"\"\n",
    "    n_tasks = len(params_list)\n",
    "    sim_matrix = np.zeros((n_tasks, n_tasks))\n",
    "    \n",
    "    for i in range(n_tasks):\n",
    "        for j in range(n_tasks):\n",
    "            if key_name in params_list[i] and key_name in params_list[j]:\n",
    "                vec_i = params_list[i][key_name].cpu().flatten().numpy()\n",
    "                vec_j = params_list[j][key_name].cpu().flatten().numpy()\n",
    "                \n",
    "                if np.linalg.norm(vec_i) > 0 and np.linalg.norm(vec_j) > 0:\n",
    "                    sim = np.dot(vec_i, vec_j) / (np.linalg.norm(vec_i) * np.linalg.norm(vec_j))\n",
    "                    sim_matrix[i, j] = sim\n",
    "                else:\n",
    "                    sim_matrix[i, j] = 0\n",
    "            else:\n",
    "                sim_matrix[i, j] = 0\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "def visualize_key_similarity(sim_matrix, key_name, param_type, save_dir):\n",
    "    \"\"\"可视化单个key的相似度矩阵\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(sim_matrix, dtype=bool), k=1)\n",
    "    \n",
    "    sns.heatmap(sim_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='viridis',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                cbar_kws={'shrink': 0.8},\n",
    "                fmt='.3f')\n",
    "    \n",
    "    plt.title(f'{param_type} - {key_name} Similarity Across Tasks', fontsize=14, pad=20)\n",
    "    plt.xlabel('Task Index')\n",
    "    plt.ylabel('Task Index')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f'{param_type}_{key_name}_similarity.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_summary_heatmap(all_sim_matrices, keys, param_type, save_dir):\n",
    "    \"\"\"创建所有keys的汇总热图\"\"\"\n",
    "    n_keys = len(keys)\n",
    "    n_tasks = all_sim_matrices[0].shape[0]\n",
    "    \n",
    "    # 计算每个key的平均对角线值（任务内相似度）\n",
    "    key_scores = []\n",
    "    for i, key in enumerate(keys):\n",
    "        diag_mean = np.mean(np.diag(all_sim_matrices[i]))\n",
    "        key_scores.append((key, diag_mean))\n",
    "    \n",
    "    # 按相似度排序\n",
    "    key_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 创建汇总图\n",
    "    fig, axes = plt.subplots(n_keys, 1, figsize=(12, 3 * n_keys))\n",
    "    \n",
    "    if n_keys == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (key, score) in enumerate(key_scores):\n",
    "        sim_matrix = all_sim_matrices[keys.index(key)]\n",
    "        \n",
    "        sns.heatmap(sim_matrix, \n",
    "                    ax=axes[idx],\n",
    "                    annot=True, \n",
    "                    cmap='viridis',\n",
    "                    center=0,\n",
    "                    square=True,\n",
    "                    cbar_kws={'shrink': 0.8},\n",
    "                    fmt='.3f')\n",
    "        \n",
    "        axes[idx].set_title(f'{key} (Avg Diag: {score:.3f})')\n",
    "        axes[idx].set_xlabel('Task Index')\n",
    "        axes[idx].set_ylabel('Task Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'{param_type}_all_keys_summary.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 1: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 2: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 3: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 4: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 5: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 6: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 7: Found 24 lora_A keys and 24 lora_B keys\n",
      "Task 8: Found 24 lora_A keys and 24 lora_B keys\n",
      "Unique lora_A keys: ['0.0.lora_A', '0.2.lora_A', '1.0.lora_A', '1.2.lora_A', '10.0.lora_A', '10.2.lora_A', '11.0.lora_A', '11.2.lora_A', '2.0.lora_A', '2.2.lora_A', '3.0.lora_A', '3.2.lora_A', '4.0.lora_A', '4.2.lora_A', '5.0.lora_A', '5.2.lora_A', '6.0.lora_A', '6.2.lora_A', '7.0.lora_A', '7.2.lora_A', '8.0.lora_A', '8.2.lora_A', '9.0.lora_A', '9.2.lora_A']\n",
      "Unique lora_B keys: ['0.0.lora_B', '0.2.lora_B', '1.0.lora_B', '1.2.lora_B', '10.0.lora_B', '10.2.lora_B', '11.0.lora_B', '11.2.lora_B', '2.0.lora_B', '2.2.lora_B', '3.0.lora_B', '3.2.lora_B', '4.0.lora_B', '4.2.lora_B', '5.0.lora_B', '5.2.lora_B', '6.0.lora_B', '6.2.lora_B', '7.0.lora_B', '7.2.lora_B', '8.0.lora_B', '8.2.lora_B', '9.0.lora_B', '9.2.lora_B']\n",
      "Generated similarity heatmap for lora_A key: 2.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 0.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 6.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 7.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 3.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 5.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 10.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 10.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 8.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 5.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 9.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 11.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 8.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 11.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 1.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 6.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 2.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 7.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 3.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 1.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 9.0.lora_A\n",
      "Generated similarity heatmap for lora_A key: 4.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 0.2.lora_A\n",
      "Generated similarity heatmap for lora_A key: 4.0.lora_A\n",
      "Generated similarity heatmap for lora_B key: 1.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 4.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 0.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 7.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 9.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 6.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 3.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 2.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 2.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 7.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 8.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 11.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 10.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 9.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 8.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 0.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 4.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 3.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 6.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 10.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 5.0.lora_B\n",
      "Generated similarity heatmap for lora_B key: 11.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 1.2.lora_B\n",
      "Generated similarity heatmap for lora_B key: 5.2.lora_B\n",
      "\n",
      "Analysis completed! Results saved to: logs/ours/SKIN/2/3/no_kd_orth_all_spec_lora/\n",
      "- Individual similarity heatmaps for each key\n",
      "- Summary heatmaps for all keys\n",
      "- NPZ files containing all similarity matrices\n"
     ]
    }
   ],
   "source": [
    "# 按key提取lora参数\n",
    "lora_a_by_key = []\n",
    "lora_b_by_key = []\n",
    "\n",
    "for i, state_dict in enumerate(task_best_model_ckpts):\n",
    "    lora_a, lora_b = extract_lora_parameters_by_key(state_dict)\n",
    "    lora_a_by_key.append(lora_a)\n",
    "    lora_b_by_key.append(lora_b)\n",
    "    print(f\"Task {i}: Found {len(lora_a)} lora_A keys and {len(lora_b)} lora_B keys\")\n",
    "\n",
    "# 获取所有唯一的key\n",
    "all_lora_a_keys = set()\n",
    "all_lora_b_keys = set()\n",
    "\n",
    "for params in lora_a_by_key:\n",
    "    all_lora_a_keys.update(params.keys())\n",
    "for params in lora_b_by_key:\n",
    "    all_lora_b_keys.update(params.keys())\n",
    "\n",
    "print(f\"Unique lora_A keys: {sorted(all_lora_a_keys)}\")\n",
    "print(f\"Unique lora_B keys: {sorted(all_lora_b_keys)}\")\n",
    "\n",
    "# 为每个lora_A key计算相似度矩阵\n",
    "lora_a_sim_matrices = {}\n",
    "for key in all_lora_a_keys:\n",
    "    sim_matrix = calculate_key_similarity_matrix(lora_a_by_key, key)\n",
    "    lora_a_sim_matrices[key] = sim_matrix\n",
    "    visualize_key_similarity(sim_matrix, key, 'lora_a', task_ckpt_dir)\n",
    "    print(f\"Generated similarity heatmap for lora_A key: {key}\")\n",
    "\n",
    "# 为每个lora_B key计算相似度矩阵\n",
    "lora_b_sim_matrices = {}\n",
    "for key in all_lora_b_keys:\n",
    "    sim_matrix = calculate_key_similarity_matrix(lora_b_by_key, key)\n",
    "    lora_b_sim_matrices[key] = sim_matrix\n",
    "    visualize_key_similarity(sim_matrix, key, 'lora_b', task_ckpt_dir)\n",
    "    print(f\"Generated similarity heatmap for lora_B key: {key}\")\n",
    "\n",
    "# 创建汇总图\n",
    "create_summary_heatmap(list(lora_a_sim_matrices.values()), \n",
    "                        list(lora_a_sim_matrices.keys()), \n",
    "                        'lora_a', task_ckpt_dir)\n",
    "\n",
    "create_summary_heatmap(list(lora_b_sim_matrices.values()), \n",
    "                        list(lora_b_sim_matrices.keys()), \n",
    "                        'lora_b', task_ckpt_dir)\n",
    "\n",
    "# 保存相似度矩阵数据\n",
    "np.savez(os.path.join(task_ckpt_dir, 'lora_a_similarity_matrices.npz'), **lora_a_sim_matrices)\n",
    "np.savez(os.path.join(task_ckpt_dir, 'lora_b_similarity_matrices.npz'), **lora_b_sim_matrices)\n",
    "\n",
    "print(f\"\\nAnalysis completed! Results saved to: {task_ckpt_dir}/\")\n",
    "print(\"- Individual similarity heatmaps for each key\")\n",
    "print(\"- Summary heatmaps for all keys\")\n",
    "print(\"- NPZ files containing all similarity matrices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cllora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
